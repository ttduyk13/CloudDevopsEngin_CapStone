version: 2.1

commands:
  install-awscli:
    steps:
      - run:
          name: Install awscli
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
            aws --version
  install-kubectl:
    steps:
      - run:
          name: Install kubectl
          command: |
            curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.26.8/2023-09-14/bin/linux/amd64/kubectl
            chmod +x ./kubectl
            mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
            echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
            kubectl version --short --client
  install-eksctl:
    steps:
      - run:
          name: Install eksctl
          command: |
            ARCH=amd64
            PLATFORM=$(uname -s)_$ARCH
            curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
            tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
            sudo mv /tmp/eksctl /usr/local/bin
  setup-venv:
    steps:
      - run:
          name: Setup venv
          command: |
            make setup
            pwd
  access-venv:
    steps:
      - run:
          name: Access venv
          command: |
            . ~/.devops/bin/activate
            pwd
  install-ansible:
    steps:
      - run:
          name: Install ansible
          command: |
            sudo apt-get install -y ansible
  destroy-environment:
    parameters:
      workflow_id:
        type: string
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            aws cloudformation delete-stack \
            --stack-name udapeople-backend-<<parameters.workflow_id>>
            aws s3 rm s3://udapeople-<<parameters.workflow_id>> --recursive
            aws cloudformation delete-stack \
            --stack-name udapeople-frontend-<<parameters.workflow_id>>

jobs:
  build:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - restore_cache:
          keys: [raw-build]
      - setup-venv
      - access-venv
      - run:
          name: install
          command: |
            make install
      - save_cache:
          paths: [~/.devops]
          key: raw-build

  test:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - restore_cache:
          keys: [raw-build]
      - access-venv
      - run:
          name: lint check
          command: |
            
            make lint
            make test

  setup-cluster:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - install-awscli
      - install-kubectl
      - install-eksctl
      - run:
          name: Setup eks
          command: |
            export CLUSTER_NAME="duytt10-clouddevopsengin-capstone-cluster"
            echo "cluster name: ${CLUSTER_NAME}"
            echo $(aws eks describe-cluster --name $CLUSTER_NAME --region us-east-1  2>&1)
            cluster_info=$(aws eks describe-cluster --name $CLUSTER_NAME --region us-east-1  2>&1 | grep -i -w "ResourceNotFoundException")
            echo $cluster_info

            if [[ $cluster_info ]]; then
                echo "The cluster $CLUSTER_NAME does not exist"
                eksctl create cluster -f .circleci/cluster/cluster.yml
                kubectl get nodes
            else
                echo "The cluster $CLUSTER_NAME exists"  
            fi

  build-docker:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - run:
          name: docker with aws ecr
          command: |
            bash upload_docker.sh

# Invoke jobs via workflows
# See: https://circleci.com/docs/configuration-reference/#workflows

workflows:
  default:
    jobs:
      - setup-cluster
      - build
      - test:
          requires: [build]
